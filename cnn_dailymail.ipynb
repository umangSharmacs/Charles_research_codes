{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "cnn_dailymail.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umangSharmacs/Charles_research_codes/blob/main/cnn_dailymail.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3351987"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "from nltk.tag import pos_tag # for proper noun\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd\n",
        "import math\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import pytrends\n",
        "from pytrends.request import TrendReq\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tag import DefaultTagger\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from scipy import spatial\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from bert_serving.client import BertClient\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "id": "b3351987",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ca3370"
      },
      "source": [
        "import os\n",
        "\n",
        "corpus=[]\n",
        "summary=[]\n",
        "path=\"C:\\\\Users\\\\umang\\\\Desktop\\\\Research\\\\cnn\\\\stories\"\n",
        "\n",
        "for file in os.listdir(path):\n",
        "    with open(f\"{path}\\\\{file}\", 'r', encoding='utf-8') as f:\n",
        "        corpus.append(f.read())\n",
        "\n",
        "articles=[]\n",
        "summary=[]\n",
        "for article in corpus:\n",
        "    txt=[]\n",
        "    for index,sentence in enumerate(article.split('\\n')):\n",
        "        if sentence=='@highlight':\n",
        "            break\n",
        "        if sentence!='':\n",
        "            txt.append(sentence)\n",
        "    s='. '.join(txt)\n",
        "    if s!='':\n",
        "        articles.append('. '.join(txt))\n",
        "    else:\n",
        "        continue\n",
        "    summ=[]\n",
        "    for s_index in range(index,len(article.split('\\n'))):\n",
        "        if article.split('\\n')[s_index]!='@highlight' and article.split('\\n')[s_index]!='':\n",
        "            summ.append(article.split('\\n')[s_index])\n",
        "    summary.append('. '.join(summ))"
      ],
      "id": "16ca3370",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ee7995a"
      },
      "source": [
        "def preprocess(text):\n",
        "    \n",
        "    sent_tokens = nltk.sent_tokenize(text)\n",
        "   \n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    word_tokens_lower=[word.lower() for word in word_tokens]\n",
        "    \n",
        "    stopWords = list(set(stopwords.words(\"english\")))\n",
        "    word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n",
        "    #print(len(word_tokens_refined))\n",
        "    \n",
        "    stem = []\n",
        "    ps = PorterStemmer()\n",
        "    for w in word_tokens_refined:\n",
        "        stem.append(ps.stem(w))\n",
        "    word_tokens_refined=stem\n",
        "    \n",
        "    return text,sent_tokens,word_tokens,word_tokens_lower,word_tokens_refined"
      ],
      "id": "1ee7995a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b071f3f1"
      },
      "source": [
        "def frequency(word_tokens_refined, sent_tokens):\n",
        "    ps = PorterStemmer()\n",
        "    freqTable = {}\n",
        "    for word in word_tokens_refined:    \n",
        "        if word in freqTable:         \n",
        "            freqTable[word] += 1    \n",
        "        else:         \n",
        "            freqTable[word] = 1\n",
        "    for k in freqTable.keys():\n",
        "        freqTable[k]= math.log10(1+freqTable[k])\n",
        "    #Compute word frequnecy score of each sentence\n",
        "    word_frequency={}\n",
        "    for sentence in sent_tokens:\n",
        "        word_frequency[sentence]=0\n",
        "        e=nltk.word_tokenize(sentence)\n",
        "        f=[]\n",
        "        for word in e:\n",
        "            f.append(ps.stem(word))\n",
        "        for word,freq in freqTable.items():\n",
        "            if word in f:\n",
        "                word_frequency[sentence]+=freq\n",
        "    maximum=max(word_frequency.values())\n",
        "    for key in word_frequency.keys():\n",
        "        try:\n",
        "            word_frequency[key]=word_frequency[key]/maximum\n",
        "            word_frequency[key]=round(word_frequency[key],3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "#    print(word_frequency.values())\n",
        "    return freqTable, word_frequency"
      ],
      "id": "b071f3f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4210036"
      },
      "source": [
        "def numerical_data(sent_tokens):\n",
        "    numeric_data={}\n",
        "    for sentence in sent_tokens:\n",
        "        numeric_data[sentence] = 0\n",
        "        word_tokens = nltk.word_tokenize(sentence)\n",
        "        for k in word_tokens:\n",
        "            if k.isdigit():\n",
        "                numeric_data[sentence] += 1\n",
        "    max_freq=max(numeric_data.values())\n",
        "    for k in numeric_data.keys():\n",
        "        try:\n",
        "            numeric_data[k] = numeric_data[k]/max_freq\n",
        "            numeric_data[k] = round(numeric_data[k], 3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "    return numeric_data"
      ],
      "id": "d4210036",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdcabca4"
      },
      "source": [
        "def sentence_length(sent_tokens):\n",
        "    sent_len_score={}\n",
        "    for sentence in sent_tokens:\n",
        "        sent_len_score[sentence] = 0\n",
        "        word_tokens = nltk.word_tokenize(sentence)\n",
        "        if len(word_tokens) in range(0,10):\n",
        "            sent_len_score[sentence]=1-0.05*(10-len(word_tokens))\n",
        "        elif len(word_tokens) in range(7,20):                    # calculate ideal sentence length using some other criteria? \n",
        "            sent_len_score[sentence]=1\n",
        "        else:\n",
        "            sent_len_score[sentence]=1-(0.05)*(len(word_tokens)-20)\n",
        "    for k in sent_len_score.keys():\n",
        "        sent_len_score[k]=round(sent_len_score[k],4)\n",
        "    #print(sent_len_score.values())\n",
        "    return sent_len_score"
      ],
      "id": "cdcabca4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcf5104b"
      },
      "source": [
        "def sentence_position(sent_tokens):\n",
        "    sentence_position={}\n",
        "    d=1\n",
        "    no_of_sent=len(sent_tokens)\n",
        "    for i in range(no_of_sent):\n",
        "        a=1/d\n",
        "        b=1/(no_of_sent-d+1)\n",
        "        sentence_position[sent_tokens[d-1]]=max(a,b)\n",
        "        d=d+1\n",
        "    for k in sentence_position.keys():\n",
        "        sentence_position[k]=round(sentence_position[k],3)\n",
        "#    print(sentence_position.values())\n",
        "    return sentence_position"
      ],
      "id": "fcf5104b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60250b4b"
      },
      "source": [
        " def uppercase(sent_tokens):\n",
        "    upper_case={}\n",
        "    for sentence in sent_tokens:\n",
        "        upper_case[sentence] = 0\n",
        "        word_tokens = nltk.word_tokenize(sentence)\n",
        "        for k in word_tokens:\n",
        "            if k.isupper():\n",
        "                upper_case[sentence] += 1\n",
        "    maximum_frequency = max(upper_case.values())\n",
        "    for k in upper_case.keys():\n",
        "        try:\n",
        "            upper_case[k] = (upper_case[k]/maximum_frequency)\n",
        "            upper_case[k] = round(upper_case[k], 3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "#    print(upper_case.values())\n",
        "    return upper_case"
      ],
      "id": "60250b4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215851e4"
      },
      "source": [
        "def propernouns(sent_tokens):\n",
        "    proper_noun={}\n",
        "    for sentence in sent_tokens:\n",
        "        tagged_sent = pos_tag(sentence.split())\n",
        "        propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
        "        proper_noun[sentence]=len(propernouns)\n",
        "    maximum_frequency = max(proper_noun.values())\n",
        "    for k in proper_noun.keys():\n",
        "        try:\n",
        "            proper_noun[k] = (proper_noun[k]/maximum_frequency)\n",
        "            proper_noun[k] = round(proper_noun[k], 3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "#    print(proper_noun.values())\n",
        "    return proper_noun"
      ],
      "id": "215851e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c357c41"
      },
      "source": [
        "def vectorize(word2vec, sentence):\n",
        "    vec=0\n",
        "    for word in sentence:\n",
        "        if word in word2vec.wv.vocab:\n",
        "            vec+= word2vec.wv[word]\n",
        "    return vec"
      ],
      "id": "1c357c41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84a3c2ce"
      },
      "source": [
        ""
      ],
      "id": "84a3c2ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79eb8846"
      },
      "source": [
        "def sentence_similarity(all_words,test_article,test_summ):\n",
        "    # train model \n",
        "    word2vec = Word2Vec(all_words, min_count=2)\n",
        "    \n",
        "    summary_vectors=[]\n",
        "    for sentence in test_summ:\n",
        "        summary_vectors.append(vectorize(word2vec,sentence))\n",
        "\n",
        "    label={}\n",
        "    for sentence in test_article:\n",
        "        label[sentence]=0\n",
        "    \n",
        "    for vector in summary_vectors:\n",
        "        sim=[]\n",
        "        for sentence in test_article:\n",
        "            vec=0\n",
        "            for word in sentence:\n",
        "                if word in word2vec.wv.vocab:\n",
        "                    vec+=word2vec.wv[word]\n",
        "            sim.append(1 - spatial.distance.cosine(vec, vector))\n",
        "        label[test_article[sim.index(max(sim))]]=1\n",
        "        sim[sim.index(max(sim))]=0\n",
        "        label[test_article[sim.index(max(sim))]]=1\n",
        "    \n",
        "#     for sentence in test_article:\n",
        "#         if sentence in label:\n",
        "#             continue\n",
        "#         vec=0\n",
        "#         label_check=False\n",
        "#         for word in sentence:\n",
        "#             if word in word2vec.wv.vocab:\n",
        "#                 vec+=word2vec.wv[word]\n",
        "#         for sumvec in summary_vectors:\n",
        "#             similarity=1 - spatial.distance.cosine(vec, sumvec)\n",
        "#             if similarity>0.93:\n",
        "#                 label[sentence]=1\n",
        "#                 label_check=True\n",
        "#                 break\n",
        "#         if label_check==False:\n",
        "#             label[sentence]=0\n",
        "    return label"
      ],
      "id": "79eb8846",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca7231fc",
        "outputId": "092eb21f-34ec-4c7b-8bce-3d13fa2aebbe"
      },
      "source": [
        "df_model=pd.DataFrame(columns=['Numerical_Data',\n",
        "                               'Sentence_length',\n",
        "                               'Sentence_position',\n",
        "                               #'Word_frequency',\n",
        "                               'Uppercase',\n",
        "                               'Proper_nouns',\n",
        "                               #'Headmatch',\n",
        "                               #'Sentence_similarity',\n",
        "                               #'TFIDF',\n",
        "                               'Label',\n",
        "                               'Key'])\n",
        "print(df_model)"
      ],
      "id": "ca7231fc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Numerical_Data, Sentence_length, Sentence_position, Uppercase, Proper_nouns, Label, Key]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4252c7e5",
        "outputId": "c72f88b5-be9e-4a55-c420-5586da09865e"
      },
      "source": [
        "for index,file in enumerate(articles[90000:]):\n",
        "    clear_output(wait=True)\n",
        "    print(index,\".......................\", end=\" \")\n",
        "    text=articles[index]\n",
        "    text1=summary[index]\n",
        "    sent_tokens = nltk.sent_tokenize(text)\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    sent_tokens1 = nltk.sent_tokenize(text1)\n",
        "    word_tokens1 = nltk.word_tokenize(text1)\n",
        "    word_tokens_lower=[word.lower() for word in word_tokens]\n",
        "    stopWords = list(set(stopwords.words(\"english\")))\n",
        "    word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n",
        "\n",
        "    numerical_data_dict=numerical_data(sent_tokens)\n",
        "\n",
        "    sentence_length_dict=sentence_length(sent_tokens)\n",
        " \n",
        "    sentence_position_dict=sentence_position(sent_tokens)\n",
        "\n",
        "    freqTable, word_frequency_dict=frequency(word_tokens_refined, sent_tokens)\n",
        "\n",
        "    uppercase_dict=uppercase(sent_tokens)\n",
        "\n",
        "    propernouns_dict=propernouns(sent_tokens)\n",
        "\n",
        "#    sent_similarity_dict=sentence_similarity(sent_tokens)\n",
        "\n",
        "#    tfidf_score=tfidf(sent_tokens,index+1)\n",
        "    \n",
        "    Label=sentence_similarity(word_tokens_refined,sent_tokens, sent_tokens1)\n",
        "\n",
        "    df_model=df_model.append(pd.DataFrame({\n",
        "        'Numerical_Data':list(numerical_data_dict.values()),\n",
        "        'Sentence_length': list(sentence_length_dict.values()),\n",
        "        'Sentence_position':list(sentence_position_dict.values()),\n",
        "        'Word_frequency':list(word_frequency_dict.values()),\n",
        "        'Uppercase':list(uppercase_dict.values()),\n",
        "        'Proper_nouns':list(propernouns_dict.values()),\n",
        "        #'TFIDF':list(tfidf_score.values()),\n",
        "        'Label':list(Label.values()),\n",
        "        'Key':list(numerical_data_dict.keys())})\n",
        "                              ,ignore_index=True)\n",
        "    \n",
        "    print(' : done')"
      ],
      "id": "4252c7e5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2464 .......................  : done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44125212",
        "outputId": "919bf5fb-ec8d-400c-8f3e-f5ea48dc1d6b"
      },
      "source": [
        "df_model['Label'].sum()"
      ],
      "id": "44125212",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13874"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15e22a48"
      },
      "source": [
        "df_model.to_csv('cnn_10.csv')"
      ],
      "id": "15e22a48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc98beef"
      },
      "source": [
        "def compute_results(x_train, y_train,x_test, y_test):\n",
        "    y_pred=[]\n",
        "    models=[]\n",
        "    # KNN\n",
        "    for i in [3,5,10,15,20,50]:\n",
        "        neigh = KNeighborsClassifier(n_neighbors=i)\n",
        "        neigh.fit(x_train, y_train)\n",
        "        y_pred.append(neigh.predict(x_test))\n",
        "        models.append(f'KNN_{i}')\n",
        "        print(f'KNN_{i} done')\n",
        "        \n",
        "    # Logistic Regression\n",
        "    clf = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
        "    y_pred.append(clf.predict(x_test))\n",
        "    models.append('Logistic_Regression')\n",
        "    print('Logistic Regression done')\n",
        "    \n",
        "    # SVM\n",
        "#     clf = svm.SVC()\n",
        "#     clf.fit(x_train, y_train)\n",
        "#     y_pred.append(clf.predict(x_test))\n",
        "#     models.append('SVM')\n",
        "#     print('SVM done')\n",
        "    \n",
        "    # Random Forest\n",
        "    for i in range(10,110,10):\n",
        "        classifier = RandomForestClassifier(n_estimators = i,criterion='entropy',random_state=0)\n",
        "        classifier.fit(x_train, y_train)\n",
        "        y_pred.append(classifier.predict(x_test))\n",
        "        models.append(f'Random_Forest_{i}')\n",
        "        print(f'Random_Forest_{i} done')\n",
        "    acc=[]\n",
        "    f1=[]\n",
        "    cm=[]\n",
        "    for index,i in enumerate(y_pred):\n",
        "        cm.append(confusion_matrix(y_test, y_pred[index]))\n",
        "        acc.append(metrics.accuracy_score(y_test, y_pred[index])*100)\n",
        "        f1.append(metrics.f1_score(y_test, y_pred[index])*100)\n",
        "    results=pd.DataFrame()\n",
        "    results['Models']=models\n",
        "    results['Accuracy']=acc\n",
        "    results['F1 score']=f1\n",
        "    results['Confusion Matrix']=cm\n",
        "    return results"
      ],
      "id": "dc98beef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27c26c00"
      },
      "source": [
        "df_model=pd.read_csv('cnn_2.csv')\n",
        "df_model.head()"
      ],
      "id": "27c26c00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "785c6818"
      },
      "source": [
        "x=df_model.drop(['Label', 'Key'], axis=1)\n",
        "y=df_model.Label.astype('int')\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state=0)\n",
        "results=compute_results(x_train, y_train,x_test, y_test)"
      ],
      "id": "785c6818",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "133f67ec"
      },
      "source": [
        "results=results.sort_values('Accuracy', ascending=False)\n",
        "results"
      ],
      "id": "133f67ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "237894d0"
      },
      "source": [
        "clf = svm.SVC()\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred.append(clf.predict(x_test))\n",
        "models.append('SVM')\n",
        "print('SVM done')"
      ],
      "id": "237894d0",
      "execution_count": null,
      "outputs": []
    }
  ]
}